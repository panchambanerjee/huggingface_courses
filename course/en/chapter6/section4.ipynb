{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwN3eIBgv7Ey"
      },
      "source": [
        "# Normalization and pre-tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpobY16Hv7Ez"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0Qe18APv7E0"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26OAM4SSv7E0",
        "outputId": "2877baa8-218f-4e9a-a1b9-3f16898dcfe4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<class 'tokenizers.Tokenizer'>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(type(tokenizer.backend_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzKDPkn4v7E1",
        "outputId": "940b0f42-e7ce-49f8-8fd9-28e23a041e26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIji70FTv7E1",
        "outputId": "f1d4791c-9ec5-4484-df76-807f84cb5bcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMZc6miMv7E1",
        "outputId": "91944c4f-1e8a-4acd-f02a-68e924ca49bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),\n",
              " ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyK6-CNHv7E2",
        "outputId": "57c4d4a9-cb3f-42b4-f856-5fc88688513f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Normalization and pre-tokenization",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}